{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85169ac6",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cec1e9b",
   "metadata": {},
   "source": [
    "## 1.1 Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafc7f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUrls(start, stop): \n",
    "\n",
    "    urls = []\n",
    "    for i in range(start, stop):\n",
    "        url = 'https://myanimelist.net/topanime.php?limit='+str(i*50)\n",
    "        r = requests.get(url)\n",
    "        html_content = r.text\n",
    "        soup = bs(html_content, 'lxml')\n",
    "        links = soup.find_all('h3') \n",
    "\n",
    "        for anime in links[:-3]:\n",
    "            if anime.find('a'):\n",
    "                urls.append(anime.find('a')['href'])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9892f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"topAnime.txt\", 'w', encoding=\"utf8\")\n",
    "f.write('\\n'.join(getUrls(0,400)))\n",
    "f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1708c59",
   "metadata": {},
   "source": [
    "## 1.2 Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec34a2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveHtml(page):\n",
    "#saving the HTML file of page 'page' in the corresponding folder\n",
    "\n",
    "    subfolder = \"downloaded_Html/page_{}\".format(page)\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "    f = open(\"topAnime.txt\", 'r', encoding=\"utf8\")\n",
    "    lines = f.readlines()[(page-1)*50:(page)*50]\n",
    "    lines = [line.rstrip() for line in lines]\n",
    "    f.close\n",
    "\n",
    "    i = 1+50*(page-1)\n",
    "    for link in lines:\n",
    "        html = requests.get(link)\n",
    "        \n",
    "        if html.status_code != 200:\n",
    "            while(html.status_code != 200): \n",
    "                check = requests.get(link)\n",
    "        \n",
    "        file_name = '{}/{}.html'.format(subfolder, i)\n",
    "        g = open(file_name, 'w', encoding=\"utf8\")\n",
    "        g.write(html.text)\n",
    "        g.close\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887957b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(379,400):\n",
    "    saveHtml(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e7ca79",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitle(anime):\n",
    "    return anime.strong.contents[0]\n",
    "\n",
    "def getType(anime):\n",
    "    return anime.find(text = 'Type:').find_next('a').contents[0]\n",
    "\n",
    "def getNumEpis(anime):\n",
    "    if anime.find(text = 'Episodes:').next_element.strip() != \"Unknown\":\n",
    "        return int(anime.find(text = 'Episodes:').next_element.strip())\n",
    "    else :\n",
    "        return []\n",
    "    \n",
    "def getStart(anime):\n",
    "    date = anime.find(text = 'Aired:').next_element.strip()\n",
    "    \n",
    "    if date != \"Not available\":\n",
    "        if len(date.split(\" to \")[0]) > 8:\n",
    "            return dt.strptime(date.split(\" to \")[0], '%b %d, %Y' )\n",
    "        elif len(date.split(\" to \")[0]) == 4:\n",
    "            return dt.strptime(date.split(\" to \")[0], '%Y' )\n",
    "        elif len(date.split(\" to \")[0]) == 8:\n",
    "            return dt.strptime(date.split(\" to \")[0], '%b %Y' )\n",
    "        else:\n",
    "            return pd.to_datetime(np.NaN, errors='coerce')\n",
    "    else :\n",
    "        return []\n",
    "    \n",
    "def getEnd(anime):\n",
    "    date = anime.find(text = 'Aired:').next_element.strip()\n",
    "    \n",
    "    if date != \"Not available\":\n",
    "        if len(date)>12 and len(date.split(\" to \")[1]) > 8 and date.split(\" to \")[1] != \"?\":\n",
    "            return dt.strptime(date.split(\" to \")[1], '%b %d, %Y' )\n",
    "        elif len(date)>12 and len(date.split(\" to \")[1]) == 4 and date.split(\" to \")[1] != \"?\":\n",
    "            return dt.strptime(date.split(\" to \")[1], '%Y' )\n",
    "        elif len(date)>12 and len(date.split(\" to \")[1]) == 8 and date.split(\" to \")[1] != \"?\":\n",
    "            return dt.strptime(date.split(\" to \")[1], '%b %Y' )\n",
    "        else:\n",
    "            return pd.to_datetime(np.NaN, errors='coerce')\n",
    "    else :\n",
    "        return []        \n",
    "    \n",
    "def getNumMemb(anime):\n",
    "    animeNumMembers = anime.find(text = 'Members:').next_element\n",
    "    return int(animeNumMembers.replace('n', '').replace(',', '').strip())\n",
    "\n",
    "def getScore(anime):\n",
    "    if anime.find(text = 'Score:').find_next('span').contents[0] != 'N/A':\n",
    "        animeScore = anime.find(text = 'Score:').find_next('span').contents\n",
    "        return float(animeScore[0])\n",
    "    else :\n",
    "        return []         \n",
    "    \n",
    "def getUsers(anime):\n",
    "    animeUsers = anime.find(text = 'Score:').find_next('span').find_next('span').contents\n",
    "    if animeUsers[0] != 'Ranked:':\n",
    "        return int(animeUsers[0])\n",
    "    else :\n",
    "        return []      \n",
    "\n",
    "def getRank(anime):\n",
    "    animeRank = anime.find(text = 'Ranked:').next_element\n",
    "    if animeRank.replace('\\n', '').strip() != 'N/A':\n",
    "        return int(animeRank.replace('\\n', '').replace('#', '').strip())\n",
    "    else :\n",
    "        return [] \n",
    "    \n",
    "def getPopularity(anime):\n",
    "    animePopularity = anime.find(text='Popularity:').next_element\n",
    "    return int(animePopularity.replace(\"\\n\",\"\").replace('#', '').strip())\n",
    "\n",
    "def getDescription(anime):\n",
    "    animeDescription = anime.find(text = 'Synopsis').find_next('p').text\n",
    "    return animeDescription.replace(\"\\n\",\"\")\n",
    "\n",
    "def getRelated(anime):\n",
    "    table = anime.find(text = 'Related Anime')\n",
    "    animeRelated = []\n",
    "\n",
    "    if(table != None):    \n",
    "        table = table.find_next('table')\n",
    "        table = table.find_all('a')\n",
    "\n",
    "        for el in table:\n",
    "            animeRelated.append(el.text)\n",
    "                \n",
    "    return animeRelated\n",
    "\n",
    "def getCharact(anime):\n",
    "    table = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "    table = table.find_all('table')\n",
    "\n",
    "    animeChar = []\n",
    "\n",
    "    for el in table:\n",
    "        people = el.find_all('h3')\n",
    "        for person in people:\n",
    "            animeChar.append(person.text)\n",
    "        \n",
    "    return animeChar\n",
    "\n",
    "def getVoices(anime):\n",
    "    table = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "    table = table.find_all('h3')\n",
    "\n",
    "    animeVoices = []\n",
    "\n",
    "    for el in table:\n",
    "        people = el.find_next('table')\n",
    "        for person in people:\n",
    "            animeVoices.append(person.find('a').text)\n",
    "        \n",
    "    return animeVoices\n",
    "\n",
    "def getStaff(anime):\n",
    "    \n",
    "    table = anime.find_all(text = 'Staff')[1].find_next(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "    animeStaff = []\n",
    "    if(table != None):    \n",
    "        table = table.find_all(\"table\")\n",
    "        for el in table:\n",
    "            x = el.find_all(\"td\")[1]\n",
    "            person = [x.find(\"a\").text, x.find(\"small\").text]\n",
    "            animeStaff.append(person)\n",
    "    \n",
    "    return animeStaff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = sorted(os.listdir('downloaded_Html')[1:], key = lambda page : int(page.split(\"_\")[1]))\n",
    "\n",
    "title = []\n",
    "typ = []\n",
    "numEpisode = []\n",
    "start = []\n",
    "end = []\n",
    "numMembers = []\n",
    "score = []\n",
    "users = []\n",
    "rank = []\n",
    "popularity = []\n",
    "synopsis = []\n",
    "related = []\n",
    "char = []\n",
    "voices = []\n",
    "staff = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f701bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    htmls = os.listdir('downloaded_Html/{}'.format(page))\n",
    "    for i in range(1,1+len(htmls)):\n",
    "        f = open(\"downloaded_Html/{}/{}.html\".format(page,50*(int(page.split(\"_\")[1])-1)+i), 'r', encoding=\"utf8\")\n",
    "        anime = bs(f, 'lxml')\n",
    "        title.append(getTitle(anime))\n",
    "        typ.append(getType(anime))\n",
    "        numEpisode.append(getNumEpis(anime))\n",
    "        start.append(getStart(anime))\n",
    "        end.append(getEnd(anime))\n",
    "        numMembers.append(getNumMemb(anime))\n",
    "        score.append(getScore(anime))\n",
    "        users.append(getUsers(anime))\n",
    "        rank.append(getRank(anime))\n",
    "        popularity.append(getPopularity(anime))\n",
    "        synopsis.append(getDescription(anime))\n",
    "        related.append(getRelated(anime))\n",
    "        char.append(getCharact(anime))\n",
    "        voices.append(getVoices(anime))\n",
    "        staff.append(getStaff(anime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51a4b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['animeTitle', 'animeType', 'animeNumEpisode','releaseDate', 'endDate', 'animeNumMembers', 'animeScore', 'animeUsers', \n",
    "       'animeRank', 'animePopularity', 'animeDescription', 'animeRelated', 'animeCharacters', 'animeVoices', 'animeStaff']\n",
    "\n",
    "types = {'animeTitle' : 'object', \n",
    "         'animeType' : 'object', \n",
    "         'animeNumEpisode' : 'int64',\n",
    "         'releaseDate' : 'datetime64', \n",
    "         'endDate' : 'datetime64', \n",
    "         'animeNumMembers' : 'int64', \n",
    "         'animeScore' : 'float64',\n",
    "         'animeUsers' : 'int64', \n",
    "         'animeRank' : 'int64',\n",
    "         'animePopularity' : 'int64',\n",
    "         'animeDescription' : 'object',\n",
    "         'animeRelated' : 'object',\n",
    "         'animeCharacters' : 'object',\n",
    "         'animeVoices' : 'object',\n",
    "         'animeStaff' : 'object'}\n",
    "\n",
    "df = pd.DataFrame(columns = col).astype(dtype = types) \n",
    "\n",
    "df.animeTitle = title \n",
    "df.animeType = typ \n",
    "df.animeNumEpisode = numEpisode \n",
    "df.releaseDate = start \n",
    "df.endDate = end \n",
    "df.animeNumMembers = numMembers \n",
    "df.animeScore = score \n",
    "df.animeUsers = users \n",
    "df.animeRank = rank \n",
    "df.animePopularity = popularity \n",
    "df.animeDescription = synopsis \n",
    "df.animeRelated = related \n",
    "df.animeCharacters = char \n",
    "df.animeVoices = voices \n",
    "df.animeStaff = staff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07662a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.to_csv(\"data/anime.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da03364",
   "metadata": {},
   "source": [
    "# 2. Search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb2e698",
   "metadata": {},
   "source": [
    "## 2.1 Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59302bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis = pd.read_csv(\"data/anime.tsv\", sep='\\t', usecols = \"animeDescription\")\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed77ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_token(row):\n",
    "    out = list()\n",
    "    tokens = tokenizer.tokenize(row.lower())\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            out.append(ps.stem(word))\n",
    "    out = list(dict.fromkeys(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e43ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "\n",
    "for row in synopsis:\n",
    "    for word in row_token(row):\n",
    "        if word not in stop_words:\n",
    "            total.append(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b69c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = {}\n",
    "id = 0\n",
    "for tok in total:\n",
    "    voc[tok] = id\n",
    "    id+=1\n",
    "\n",
    "with open('data/vocabulary.csv', 'w', newline = '',encoding = 'utf8') as f:\n",
    "    fieldnames = ['Word', 'term_id']\n",
    "    w = csv.DictWriter(f, fieldnames = fieldnames,)\n",
    "    w.writeheader()\n",
    "    for key in voc:\n",
    "        w.writerow({\"Word\": key, 'term_id': voc[key]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be69d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.read_csv(\"data/vocabulary.csv\")\n",
    "vocabulary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76492f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setDict(d,l):\n",
    "    for i in l:\n",
    "        d[i] = []\n",
    "    return d\n",
    "\n",
    "def listToString(l):\n",
    "    return ' '.join([str(elem) for elem in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf516ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "inverted_index = setDict(inverted_index, vocabulary.term_id)\n",
    "\n",
    "for i in range(len(synopsis)):\n",
    "    s = listToString(row_token(synopsis[i]))\n",
    "    for j in range(len(vocabulary.Word)):\n",
    "        if s.find(str(vocabulary.Word[j])) != -1:\n",
    "            inverted_index[vocabulary.term_id[j]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ade0c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/inverted_index.csv', 'w', newline = '',encoding = 'utf8') as f:\n",
    "    fieldnames = ['term_id', 'document']\n",
    "    w = csv.DictWriter(f, fieldnames = fieldnames,)\n",
    "    w.writeheader()\n",
    "    for key in inverted_index:\n",
    "        w.writerow({\"term_id\": key, 'document': inverted_index[key]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchWord(i):\n",
    "    vocabulary = pd.read_csv(\"data/vocabulary.csv\")\n",
    "    return i in list(vocabulary.Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362ad3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchEngine(query):\n",
    "    vocabulary = pd.read_csv(\"data/vocabulary.csv\")\n",
    "    index = pd.read_csv(\"data/inverted_index.csv\")\n",
    "    stemQuery = row_token(query)\n",
    "    \n",
    "    if all([searchWord(i) for i in stemQuery]):\n",
    "        ind = [np.where(vocabulary.Word == x)[0][0] for x in stemQuery]\n",
    "        aux = [set(ast.literal_eval(index.document[vocabulary.term_id[i]])) for i in ind]\n",
    "        return set.intersection(*aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e676777",
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentFinder(query):\n",
    "    anime = pd.read_csv(\"data/anime.tsv\", sep='\\t', usecols = [\"animeTitle\",\"animeDescription\"])\n",
    "    f = open(\"data/topAnime.txt\", 'r', encoding=\"utf8\")\n",
    "    topAnimeUrls = f.readlines()\n",
    "    col = ['animeTitle', \"animeDescription\", \"Url\"]\n",
    "    df = pd.DataFrame(columns = col)\n",
    "    ind = searchEngine(query)\n",
    "        \n",
    "    df.animeTitle = [anime.animeTitle[i] for i in ind]\n",
    "    df.animeDescription = [anime.animeDescription[i] for i in ind]\n",
    "    df.Url = [topAnimeUrls[i] for i in ind]\n",
    "    \n",
    "    f.close\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d4d43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"race sayan\"\n",
    "documentFinder(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e30c53",
   "metadata": {},
   "source": [
    "## 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e6ced8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "import ast\n",
    "from nltk.corpus import stopwords\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime as dt\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from os.path import expanduser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d511458",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordnet_pos_gen(lista):\n",
    "    pos_tags = list(nltk.pos_tag(lista))\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    t = []\n",
    "    for x in pos_tags:\n",
    "        try:\n",
    "            t.append((x[0], tag_dict[x[1][0]]))\n",
    "        except Exception:\n",
    "            t.append(\n",
    "                (x[0], \"n\"))  # a get around to assign irrelevant tags to one of wordnet classes, noun in this case\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002a8cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87addff7",
   "metadata": {},
   "source": [
    "# 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b0fc35",
   "metadata": {},
   "source": [
    "# 4. Understanding the anime's reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b594a734",
   "metadata": {},
   "source": [
    "# 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aef596a",
   "metadata": {},
   "source": [
    "We are askedd by a personal trainer who has a back-to-back sequence of requests for appointments, to provide a schedule that maximizes the total length of the accepted appointments avoiding consecutive ones. \n",
    "\n",
    "Our first idea is to analyze with a tree all the possible combination of the possible choices. As we can see in the photo below, until we have an array of length 5, the possible choices are forced:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdb13f9",
   "metadata": {},
   "source": [
    "![image4](images/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed95586",
   "metadata": {},
   "source": [
    "Now that we want to put the fifth element of the array, we can take into account the possibility of appending that directly to the root, that is the first element of the array, but we realize that it makes no sense because:\n",
    "- time is always positive;\n",
    "- the fifth element can be appended to the third element since they are not consecutive\n",
    "\n",
    "so when we want to append the fifth element, since we want to get maximizes the amount of time of the appointment, we put it under the third one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ec9ede",
   "metadata": {},
   "source": [
    "![image2](images/image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ac7595",
   "metadata": {},
   "source": [
    "In conclusion we get a binary tree of the shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88870b7",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src = \"images/image6.png\" width = \"500px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460a01d5",
   "metadata": {},
   "source": [
    "So in order to solve the problem assigned by the personal trainer we will have to build this type of trees. In particular we note that there are only two possible distinct casese the tree rooted in the first element and the one rooted in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857b9007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create all the possibile path from 1 to n and we save it in lists\n",
    "def possiblePath(index):\n",
    "    lists = [[index[0]]]\n",
    "    n = 2\n",
    "    \n",
    "    while n < len(index):\n",
    "        for l in lists:\n",
    "            # we add the element n+1\n",
    "            if(n - np.where(index == l[-1])[0][0] == 2):\n",
    "                l.append(index[n])\n",
    "            # we add the element n+2\n",
    "            if len(l)>1 and (n - np.where(index == l[-2])[0][0] == 3):\n",
    "                aux = l[:-1]\n",
    "                aux.append(index[n]) \n",
    "                if aux not in lists:\n",
    "                    lists.append(aux)\n",
    "        n +=1\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b2443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the path of index obtained in the previous function \n",
    "# we  trasform each index in the corrispondent request value\n",
    "def pathToRequest(path, request):\n",
    "    \n",
    "    for l in path:\n",
    "        for i in range(len(l)):\n",
    "            ind = l[i]-1\n",
    "            l[i] = request[ind]\n",
    "            \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b998ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seraching for the best path between the 1-rooted tree vs the 2-rooted tree\n",
    "def optimalPath(path1,path2):\n",
    "    \n",
    "    sums1 = np.array([sum(l) for l in path1])\n",
    "    sums2 = np.array([sum(l) for l in path2])\n",
    "    \n",
    "    if(max(sums1)>=max(sums2)):\n",
    "        max_value = np.where(sums1 == max(sums1))[0][0]\n",
    "        return path1[max_value]\n",
    "    else:\n",
    "        max_value = np.where(sums2 == max(sums2))[0][0]\n",
    "        return path2[max_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c5ec30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "request = [30,40,25,50,30,20]\n",
    "index = np.array([*range(1,len(request)+1)])\n",
    "\n",
    "path1 = possiblePath(index)\n",
    "path1 = pathToRequest(path1,request)\n",
    "\n",
    "path2 = possiblePath(index[1:])\n",
    "path2 = pathToRequest(path2,request)\n",
    "\n",
    "request = optimalPath(path1,path2)\n",
    "print(\"The best way to accept the appointments in the given order is: \", request ,\"\\nwith a total duration of :\" , sum(request))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
