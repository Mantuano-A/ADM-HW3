{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Get the list of animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getUrls(start, stop): \n",
    "\n",
    "    urls = []\n",
    "    for i in range(start, stop):\n",
    "        url = 'https://myanimelist.net/topanime.php?limit='+str(i*50)\n",
    "        r = requests.get(url)\n",
    "        html_content = r.text\n",
    "        soup = bs(html_content, 'lxml')\n",
    "        links = soup.find_all('h3') \n",
    "\n",
    "        for anime in links[:-3]:\n",
    "            if anime.find('a'):\n",
    "                urls.append(anime.find('a')['href'])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"topAnime.txt\", 'w', encoding=\"utf8\")\n",
    "f.write('\\n'.join(getUrls(0,400)))\n",
    "f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Crawl animes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveHtml(page):\n",
    "#saving the HTML file of page 'page' in the corresponding folder\n",
    "\n",
    "    subfolder = \"downloaded_Html/page_{}\".format(page)\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "    f = open(\"topAnime.txt\", 'r', encoding=\"utf8\")\n",
    "    lines = f.readlines()[(page-1)*50:(page)*50]\n",
    "    lines = [line.rstrip() for line in lines]\n",
    "    f.close\n",
    "\n",
    "    i = 1+50*(page-1)\n",
    "    for link in lines:\n",
    "        html = requests.get(link)\n",
    "        \n",
    "        if html.status_code != 200:\n",
    "            while(html.status_code != 200): \n",
    "                check = requests.get(link)\n",
    "        \n",
    "        file_name = '{}/{}.html'.format(subfolder, i)\n",
    "        g = open(file_name, 'w', encoding=\"utf8\")\n",
    "        g.write(html.text)\n",
    "        g.close\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(379,400):\n",
    "    saveHtml(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitle(anime):\n",
    "    return anime.strong.contents[0]\n",
    "\n",
    "def getType(anime):\n",
    "    return anime.find(text = 'Type:').find_next('a').contents[0]\n",
    "\n",
    "def getNumEpis(anime):\n",
    "    if anime.find(text = 'Episodes:').next_element.strip() != \"Unknown\":\n",
    "        return int(anime.find(text = 'Episodes:').next_element.strip())\n",
    "    else :\n",
    "        return []\n",
    "    \n",
    "def getStart(anime):\n",
    "    date = anime.find(text = 'Aired:').next_element.strip()\n",
    "    \n",
    "    if date != \"Not available\":\n",
    "        if len(date.split(\" to \")[0]) > 8:\n",
    "            return dt.strptime(date.split(\" to \")[0], '%b %d, %Y' )\n",
    "        elif len(date.split(\" to \")[0]) == 4:\n",
    "            return dt.strptime(date.split(\" to \")[0], '%Y' )\n",
    "        elif len(date.split(\" to \")[0]) == 8:\n",
    "            return dt.strptime(date.split(\" to \")[0], '%b %Y' )\n",
    "        else:\n",
    "            return pd.to_datetime(np.NaN, errors='coerce')\n",
    "    else :\n",
    "        return []\n",
    "    \n",
    "def getEnd(anime):\n",
    "    date = anime.find(text = 'Aired:').next_element.strip()\n",
    "    \n",
    "    if date != \"Not available\":\n",
    "        if len(date)>12 and len(date.split(\" to \")[1]) > 8 and date.split(\" to \")[1] != \"?\":\n",
    "            return dt.strptime(date.split(\" to \")[1], '%b %d, %Y' )\n",
    "        elif len(date)>12 and len(date.split(\" to \")[1]) == 4 and date.split(\" to \")[1] != \"?\":\n",
    "            return dt.strptime(date.split(\" to \")[1], '%Y' )\n",
    "        elif len(date)>12 and len(date.split(\" to \")[1]) == 8 and date.split(\" to \")[1] != \"?\":\n",
    "            return dt.strptime(date.split(\" to \")[1], '%b %Y' )\n",
    "        else:\n",
    "            return pd.to_datetime(np.NaN, errors='coerce')\n",
    "    else :\n",
    "        return []        \n",
    "    \n",
    "def getNumMemb(anime):\n",
    "    animeNumMembers = anime.find(text = 'Members:').next_element\n",
    "    return int(animeNumMembers.replace('n', '').replace(',', '').strip())\n",
    "\n",
    "def getScore(anime):\n",
    "    if anime.find(text = 'Score:').find_next('span').contents[0] != 'N/A':\n",
    "        animeScore = anime.find(text = 'Score:').find_next('span').contents\n",
    "        return float(animeScore[0])\n",
    "    else :\n",
    "        return []         \n",
    "    \n",
    "def getUsers(anime):\n",
    "    animeUsers = anime.find(text = 'Score:').find_next('span').find_next('span').contents\n",
    "    if animeUsers[0] != 'Ranked:':\n",
    "        return int(animeUsers[0])\n",
    "    else :\n",
    "        return []      \n",
    "\n",
    "def getRank(anime):\n",
    "    animeRank = anime.find(text = 'Ranked:').next_element\n",
    "    if animeRank.replace('\\n', '').strip() != 'N/A':\n",
    "        return int(animeRank.replace('\\n', '').replace('#', '').strip())\n",
    "    else :\n",
    "        return [] \n",
    "    \n",
    "def getPopularity(anime):\n",
    "    animePopularity = anime.find(text='Popularity:').next_element\n",
    "    return int(animePopularity.replace(\"\\n\",\"\").replace('#', '').strip())\n",
    "\n",
    "def getDescription(anime):\n",
    "    animeDescription = anime.find(text = 'Synopsis').find_next('p').text\n",
    "    return animeDescription.replace(\"\\n\",\"\")\n",
    "\n",
    "def getRelated(anime):\n",
    "    table = anime.find(text = 'Related Anime')\n",
    "    animeRelated = []\n",
    "\n",
    "    if(table != None):    \n",
    "        table = table.find_next('table')\n",
    "        table = table.find_all('a')\n",
    "\n",
    "        for el in table:\n",
    "            animeRelated.append(el.text)\n",
    "                \n",
    "    return animeRelated\n",
    "\n",
    "def getCharact(anime):\n",
    "    table = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "    table = table.find_all('table')\n",
    "\n",
    "    animeChar = []\n",
    "\n",
    "    for el in table:\n",
    "        people = el.find_all('h3')\n",
    "        for person in people:\n",
    "            animeChar.append(person.text)\n",
    "        \n",
    "    return animeChar\n",
    "\n",
    "def getVoices(anime):\n",
    "    table = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "    table = table.find_all('h3')\n",
    "\n",
    "    animeVoices = []\n",
    "\n",
    "    for el in table:\n",
    "        people = el.find_next('table')\n",
    "        for person in people:\n",
    "            animeVoices.append(person.find('a').text)\n",
    "        \n",
    "    return animeVoices\n",
    "\n",
    "def getStaff(anime):\n",
    "    \n",
    "    table = anime.find_all(text = 'Staff')[1].find_next(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "    animeStaff = []\n",
    "    if(table != None):    \n",
    "        table = table.find_all(\"table\")\n",
    "        for el in table:\n",
    "            x = el.find_all(\"td\")[1]\n",
    "            person = [x.find(\"a\").text, x.find(\"small\").text]\n",
    "            animeStaff.append(person)\n",
    "    \n",
    "    return animeStaff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = sorted(os.listdir('downloaded_Html')[1:], key = lambda page : int(page.split(\"_\")[1]))\n",
    "\n",
    "title = []\n",
    "typ = []\n",
    "numEpisode = []\n",
    "start = []\n",
    "end = []\n",
    "numMembers = []\n",
    "score = []\n",
    "users = []\n",
    "rank = []\n",
    "popularity = []\n",
    "synopsis = []\n",
    "related = []\n",
    "char = []\n",
    "voices = []\n",
    "staff = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in pages:\n",
    "    htmls = os.listdir('downloaded_Html/{}'.format(page))\n",
    "    for i in range(1,1+len(htmls)):\n",
    "        f = open(\"downloaded_Html/{}/{}.html\".format(page,50*(int(page.split(\"_\")[1])-1)+i), 'r', encoding=\"utf8\")\n",
    "        anime = bs(f, 'lxml')\n",
    "        title.append(getTitle(anime))\n",
    "        typ.append(getType(anime))\n",
    "        numEpisode.append(getNumEpis(anime))\n",
    "        start.append(getStart(anime))\n",
    "        end.append(getEnd(anime))\n",
    "        numMembers.append(getNumMemb(anime))\n",
    "        score.append(getScore(anime))\n",
    "        users.append(getUsers(anime))\n",
    "        rank.append(getRank(anime))\n",
    "        popularity.append(getPopularity(anime))\n",
    "        synopsis.append(getDescription(anime))\n",
    "        related.append(getRelated(anime))\n",
    "        char.append(getCharact(anime))\n",
    "        voices.append(getVoices(anime))\n",
    "        staff.append(getStaff(anime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['animeTitle', 'animeType', 'animeNumEpisode','releaseDate', 'endDate', 'animeNumMembers', 'animeScore', 'animeUsers', \n",
    "       'animeRank', 'animePopularity', 'animeDescription', 'animeRelated', 'animeCharacters', 'animeVoices', 'animeStaff']\n",
    "\n",
    "types = {'animeTitle' : 'object', \n",
    "         'animeType' : 'object', \n",
    "         'animeNumEpisode' : 'int64',\n",
    "         'releaseDate' : 'datetime64', \n",
    "         'endDate' : 'datetime64', \n",
    "         'animeNumMembers' : 'int64', \n",
    "         'animeScore' : 'float64',\n",
    "         'animeUsers' : 'int64', \n",
    "         'animeRank' : 'int64',\n",
    "         'animePopularity' : 'int64',\n",
    "         'animeDescription' : 'object',\n",
    "         'animeRelated' : 'object',\n",
    "         'animeCharacters' : 'object',\n",
    "         'animeVoices' : 'object',\n",
    "         'animeStaff' : 'object'}\n",
    "\n",
    "df = pd.DataFrame(columns = col).astype(dtype = types) \n",
    "\n",
    "df.animeTitle = title \n",
    "df.animeType = typ \n",
    "df.animeNumEpisode = numEpisode \n",
    "df.releaseDate = start \n",
    "df.endDate = end \n",
    "df.animeNumMembers = numMembers \n",
    "df.animeScore = score \n",
    "df.animeUsers = users \n",
    "df.animeRank = rank \n",
    "df.animePopularity = popularity \n",
    "df.animeDescription = synopsis \n",
    "df.animeRelated = related \n",
    "df.animeCharacters = char \n",
    "df.animeVoices = voices \n",
    "df.animeStaff = staff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.to_csv(\"data/anime.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Anime Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we preprocess the Synopsis of the Anime with the following steps:\n",
    "\n",
    "- Stopwords and punctuation removal\n",
    "- Case normalization (we chose to convert everything to lower case)\n",
    "- Lemmatization, i.e., an alternative to Stemming based on *Part-Of-Speech tagging* and a dictionary (english in our case). We preferred it over Stemming because it returns proper words that have a meaning in english rather then cut off portions of those words\n",
    "- Tokenization of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from os.path import expanduser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part Of Speech Tagging Step needed for Lemmatization\n",
    "def wordnet_pos_gen(lista): \n",
    "    pos_tags = list(nltk.pos_tag(lista))\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    t = []\n",
    "    for x in pos_tags:\n",
    "        try:\n",
    "            t.append((x[0], tag_dict[x[1][0]]))\n",
    "        except Exception:\n",
    "            t.append(\n",
    "                (x[0], \"n\"))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords Removal\n",
    "def stopping(ls, *args):\n",
    "    processed = []\n",
    "    for case in ls:\n",
    "        tok = word_tokenize(case)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.add(\",\"), stop_words.add(\".\"), stop_words.add(\";\"), stop_words.add(\":\")\n",
    "        res = [x for x in tok if not x in stop_words]\n",
    "        result = \"\"\n",
    "        for x in res:\n",
    "            result = result + \" \" + x\n",
    "        processed.append(result)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer\n",
    "def lemma(lista):\n",
    "    out = []\n",
    "    t = wordnet_pos_gen(lista)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    res = [lemmatizer.lemmatize(w[0], w[1]) for w in t]\n",
    "    result = \"\"\n",
    "    for x in res:\n",
    "        result = result + \" \" + x\n",
    "    out.append(result.lower())  #case normalization step here\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of preprocessed anime descriptions\n",
    "descr_corpus_tokenized = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for descr in df.animeDescription:\n",
    "    tokenized_descr = tokenizer.tokenize(descr.lower())\n",
    "    descr_corpus_tokenized.append(tokenized_descr)\n",
    "\n",
    "for s,descr in enumerate(descr_corpus_tokenized):\n",
    "    filtered = []\n",
    "    for token in descr:\n",
    "        if len(token) > 2 and not token.isnumeric():\n",
    "            filtered.append(token)\n",
    "    descr_corpus_tokenized[s] = filtered\n",
    "\n",
    "#Create a new column with the preprocessed text, since we will output the original version    \n",
    "df[\"animeDescr\"]=\"\"\n",
    "\n",
    "#Assign the new description to each new row as a list\n",
    "for y in range(len(df.animeDescription)):\n",
    "    print(y)\n",
    "    df.animeDescr[y] = [x for x in stopping(lemma(descr_corpus_tokenized[y])) if x != \"\"]\n",
    "\n",
    "#Convert lists into strings for each row     \n",
    "for x in range(len(df.animeDescr)):\n",
    "    s = \"\"\n",
    "    for y in df.animeDescr[x]:\n",
    "        s+=y\n",
    "    df.animeDescr[x] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synopsis = pd.read_csv(\"data/anime.tsv\", sep='\\t', usecols = \"animeDescription\")\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_token(row):\n",
    "    out = list()\n",
    "    tokens = tokenizer.tokenize(row.lower())\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            out.append(ps.stem(word))\n",
    "    out = list(dict.fromkeys(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "\n",
    "for row in synopsis:\n",
    "    for word in row_token(row):\n",
    "        if word not in stop_words:\n",
    "            total.append(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc = {}\n",
    "id = 0\n",
    "for tok in total:\n",
    "    voc[tok] = id\n",
    "    id+=1\n",
    "\n",
    "with open('data/vocabulary.csv', 'w', newline = '',encoding = 'utf8') as f:\n",
    "    fieldnames = ['Word', 'term_id']\n",
    "    w = csv.DictWriter(f, fieldnames = fieldnames,)\n",
    "    w.writeheader()\n",
    "    for key in voc:\n",
    "        w.writerow({\"Word\": key, 'term_id': voc[key]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = pd.read_csv(\"data/vocabulary.csv\")\n",
    "vocabulary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setDict(d,l):\n",
    "    for i in l:\n",
    "        d[i] = []\n",
    "    return d\n",
    "\n",
    "def listToString(l):\n",
    "    return ' '.join([str(elem) for elem in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_index = {}\n",
    "inverted_index = setDict(inverted_index, vocabulary.term_id)\n",
    "\n",
    "for i in range(len(synopsis)):\n",
    "    s = listToString(row_token(synopsis[i]))\n",
    "    for j in range(len(vocabulary.Word)):\n",
    "        if s.find(str(vocabulary.Word[j])) != -1:\n",
    "            inverted_index[vocabulary.term_id[j]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/inverted_index.csv', 'w', newline = '',encoding = 'utf8') as f:\n",
    "    fieldnames = ['term_id', 'document']\n",
    "    w = csv.DictWriter(f, fieldnames = fieldnames,)\n",
    "    w.writeheader()\n",
    "    for key in inverted_index:\n",
    "        w.writerow({\"term_id\": key, 'document': inverted_index[key]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchWord(i):\n",
    "    vocabulary = pd.read_csv(\"data/vocabulary.csv\")\n",
    "    return i in list(vocabulary.Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchEngine(query):\n",
    "    vocabulary = pd.read_csv(\"data/vocabulary.csv\")\n",
    "    index = pd.read_csv(\"data/inverted_index.csv\")\n",
    "    stemQuery = row_token(query)\n",
    "    \n",
    "    if all([searchWord(i) for i in stemQuery]):\n",
    "        ind = [np.where(vocabulary.Word == x)[0][0] for x in stemQuery]\n",
    "        aux = [set(ast.literal_eval(index.document[vocabulary.term_id[i]])) for i in ind]\n",
    "        return set.intersection(*aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def documentFinder(query):\n",
    "    anime = pd.read_csv(\"data/anime.tsv\", sep='\\t', usecols = [\"animeTitle\",\"animeDescription\"])\n",
    "    f = open(\"data/topAnime.txt\", 'r', encoding=\"utf8\")\n",
    "    topAnimeUrls = f.readlines()\n",
    "    col = ['animeTitle', \"animeDescription\", \"Url\"]\n",
    "    df = pd.DataFrame(columns = col)\n",
    "    ind = searchEngine(query)\n",
    "        \n",
    "    df.animeTitle = [anime.animeTitle[i] for i in ind]\n",
    "    df.animeDescription = [anime.animeDescription[i] for i in ind]\n",
    "    df.Url = [topAnimeUrls[i] for i in ind]\n",
    "    \n",
    "    f.close\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"race sayan\"\n",
    "documentFinder(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pickle\n",
    "from nltk.tokenize import RegexpTokenizer, from numpy import inner, from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenization of Preprocessed Text\n",
    "processed_descr_corpus_tokenized = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for descr in df.animeDescr:\n",
    "    tokenized_descr = tokenizer.tokenize(descr.lower())\n",
    "    processed_descr_corpus_tokenized.append(tokenized_descr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Built an Inverted Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency of words for each Document\n",
    "tf={}\n",
    "for i in range(len(processed_descr_corpus_tokenized)):\n",
    "    tf[i] = {}\n",
    "    for w in processed_descr_corpus_tokenized[i]:\n",
    "        if w not in tf[i]:\n",
    "            tf[i][w]=0\n",
    "        tf[i][w]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Frequency of all words present in the corpus of documents\n",
    "idf={}\n",
    "for l in processed_descr_corpus_tokenized:\n",
    "    for w in l:\n",
    "        if w not in idf:\n",
    "            idf[w]=0\n",
    "        idf[w]+=1\n",
    "\n",
    "for key in idf:\n",
    "    idf[key] =  np.log10(len(df.animeTitle)/idf[key])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverted Index containing TF-IDF score\n",
    "tfidf_invIndex = {}\n",
    "for j in range(len(df.animeTitle)):\n",
    "    for w in tf[j]:\n",
    "        if w not in list(tfidf_invIndex.keys()):\n",
    "            tfidf_invIndex[w] = []\n",
    "        tfidf_invIndex[w].append((j, round(tf[j][w] / idf[w], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved Inverted Index as a pickle file   \n",
    "with open('vocabularyFinal.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_invIndex, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load previously saved Inverted Index\n",
    "with open('vocabularyFinal.pkl', 'rb') as f:\n",
    "    tfidf_invIndex = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Retrieve the most similar documents to the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Query\n",
    "query = \"sword\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency of words in the Query\n",
    "tfQ= {}\n",
    "for w in query:\n",
    "    if w not in list(tfQ.keys()):\n",
    "        tfQ[w]=0\n",
    "    tfQ[w]+=1\n",
    "    \n",
    "#Compute TFIDF values of each word in the Query    \n",
    "tfIdfQ = {}\n",
    "for w in list(tfQ.keys()):\n",
    "    tfIdfQ[w] =   tfQ[w]/np.log10(len(df.animeTitle)/idf[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select most similar documents to the query (i.e,. those containing ALL the words of the query)\n",
    "rightDocs = []\n",
    "for i in range(len(df.animeTitle)):\n",
    "    for w in query:\n",
    "        if w in list(tf[i].keys()):\n",
    "            if w==query[-1]:\n",
    "                rightDocs.append(i)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve TFIFD values of similar documents' words from the the InvertedIndex\n",
    "for i in rightDocs:\n",
    "    for word in tf[i]:\n",
    "        tfidfD = [x[1] for x in tfidf_invIndex[word] if x[0]==i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have TFIDF values for every word in both the *similar* documents and the query, we need to return the top-k ones based on the cosine similarity. For this, we create a vector representation of words, using the words as variables in the same exact order. Then, we can compute the cosine between the vectors as an approximation for similarity.\n",
    "\n",
    "We push newly computed cosine scores into a heap with k nodes, popping the minima when exceeding k nodes.\n",
    "\n",
    "The choice of k is arbitrary, we may avoid to pick a number that is either too small (e.g., 1, because that result is not the one that the user was searching we have failed) or too big (returning too many results may be cumbersome in terms of speed, and a user most likely might need just a few). Anything in between is ok. Furthermore, can set it to the exact number of results that would fit in a single page in our browser (with the assumption that the average user will rarely go the second page, let alone the n-th). In the case of google search engine for example that number is set to 10, so we made the same choice in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the most similar k documents, with k chosen as 10 to match the number of results that fit in a google page, \n",
    "#in terms of cosine similarity, handled with a heap data structure\n",
    "scoreHeap, k = [], 10\n",
    "heapq.heapify(scoreHeap)\n",
    "for i in rightDocs:\n",
    "    vector = list(set([x for x in list(tfIdfQ.keys())] + [x for x in list(tf[i].keys())]))\n",
    "    vectorQ = dict.fromkeys(vector, 0)\n",
    "    for key in tfIdfQ:\n",
    "        vectorQ[key] = tfIdfQ[key]\n",
    "    vectorD = dict.fromkeys(vector, 0)\n",
    "    for word in tf[i]:\n",
    "        vectorD[word] = [x[1] for x in tfidf_invIndex[word] if x[0] == i][0]\n",
    "    cosine = np.inner(list(vectorQ.values()), list(vectorD.values())) / (norm(list(vectorQ.values())) * norm(list(vectorD.values())))  # Best\n",
    "    heapq.heappush(scoreHeap,(i,cosine))\n",
    "    if len(scoreHeap)==k+1:\n",
    "        heapq.heappop(scoreHeap)\n",
    "    scoreHeap  = [x for x in scoreHeap if float(x[1])!=0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreHeap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output formatting\n",
    "out = []\n",
    "for x in range(10):\n",
    "    id = scoreHeap[x][0]\n",
    "    title = df.animeTitle.iloc[id]\n",
    "    des = df.animeDescription[id]\n",
    "    url = \"\"\n",
    "    sim = scoreHeap[x][1]\n",
    "    out.append((id,title,des,url,sim))\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our new metric to rank anime, since it must be created using existing variables, is made of 4 components: \n",
    "- Cosine Similarity (previously computed)\n",
    "- Number of members for each anime (corresponding to the number of users that have added the anime to the list) \n",
    "- Number of Users for each anime (corresponding to the number of users that gave a vote to the anime)\n",
    "- Anime score (a vote ranging from 1 to 10 that users of the website can assign to each anime)\n",
    "\n",
    "The cosine similarity was a score ranging already from 0 to 1 (theoretically the cosine would range from -1 to 1, but in this setting i.e. word frequencies, the support is only made of positive numbers), while the others weren't and thus were normalized dividing by their respective maximum value of the dataset.\n",
    "\n",
    "These four quantities were aggregated in such a way to obtain once again a similarity score (hence ranging from 0 to 1), trying to penalize the cosine the least with respect to the other three variables. From our data, we have noticed there are just few limit cases going a bit above one (e.g., top-3 anime for popularity or score, assuming a oddly high cosine value), we tackled the problem setting the upper bound to 1 (through a lambda function).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find highest value for variables that need to be normalized\n",
    "largestMemb = max(df.animeNumMembers) \n",
    "largestUsers =  max(df.animeUsers.apply(lambda x: pd.to_numeric(x, errors='coerce')))\n",
    "highestVote =  max(df.animeScore.apply(lambda x: pd.to_numeric(x, errors='coerce')))\n",
    "\n",
    "#Normalize variable in range [0,1]\n",
    "df[\"normMemb\"] = np.array(df.animeNumMembers) / largestMemb\n",
    "df[\"normUsers\"] = np.array(df.animeUsers.apply(lambda x: pd.to_numeric(x, errors='coerce'))) / largestUsers\n",
    "df[\"normVote\"] = np.array(df.animeScore.apply(lambda x: pd.to_numeric(x, errors='coerce'))) / highestVote\n",
    "\n",
    "#Upper Bound to 1 for cases exceeding 1\n",
    "cap = lambda x: 1.0 if x>1 else x\n",
    "cosine = 1231231 # 0.80\n",
    "score = cap(4/5 * (cosine) + 1/10 * (df.normMemb[0] + df.normUsers[0] + df.normVote[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Understanding the anime's reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codice Vecchio da cambiare\n",
    "\n",
    "#Sample Query\n",
    "query = \"death note\"\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import operator\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df[\"sentiment_score\"] = df.animeDescription.apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "df[\"sentiment\"] = np.select([df[\"sentiment_score\"] < 0, df[\"sentiment_score\"] == 0, df[\"sentiment_score\"] > 0],\n",
    "                           ['neg', 'neu', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are askedd by a personal trainer who has a back-to-back sequence of requests for appointments, to provide a schedule that maximizes the total length of the accepted appointments avoiding consecutive ones. \n",
    "\n",
    "Our first idea is to analyze with a tree all the possible combination of the possible choices. As we can see in the photo below, until we have an array of length 5, the possible choices are forced:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image4](images/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we want to put the fifth element of the array, we can take into account the possibility of appending that directly to the root, that is the first element of the array, but we realize that it makes no sense because:\n",
    "- time is always positive;\n",
    "- the fifth element can be appended to the third element since they are not consecutive\n",
    "\n",
    "so when we want to append the fifth element, since we want to get maximizes the amount of time of the appointment, we put it under the third one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image2](images/image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion we get a binary tree of the shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src = \"images/image6.png\" width = \"500px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in order to solve the problem assigned by the personal trainer we will have to build this type of trees. In particular we note that there are only two possible distinct casese the tree rooted in the first element and the one rooted in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create all the possibile path from 1 to n and we save it in lists\n",
    "def possiblePath(index):\n",
    "    lists = [[index[0]]]\n",
    "    n = 2\n",
    "    \n",
    "    while n < len(index):\n",
    "        for l in lists:\n",
    "            # we add the element n+1\n",
    "            if(n - np.where(index == l[-1])[0][0] == 2):\n",
    "                l.append(index[n])\n",
    "            # we add the element n+2\n",
    "            if len(l)>1 and (n - np.where(index == l[-2])[0][0] == 3):\n",
    "                aux = l[:-1]\n",
    "                aux.append(index[n]) \n",
    "                if aux not in lists:\n",
    "                    lists.append(aux)\n",
    "        n +=1\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the path of index obtained in the previous function \n",
    "# we  trasform each index in the corrispondent request value\n",
    "def pathToRequest(path, request):\n",
    "    \n",
    "    for l in path:\n",
    "        for i in range(len(l)):\n",
    "            ind = l[i]-1\n",
    "            l[i] = request[ind]\n",
    "            \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seraching for the best path between the 1-rooted tree vs the 2-rooted tree\n",
    "def optimalPath(path1,path2):\n",
    "    \n",
    "    sums1 = np.array([sum(l) for l in path1])\n",
    "    sums2 = np.array([sum(l) for l in path2])\n",
    "    \n",
    "    if(max(sums1)>=max(sums2)):\n",
    "        max_value = np.where(sums1 == max(sums1))[0][0]\n",
    "        return path1[max_value]\n",
    "    else:\n",
    "        max_value = np.where(sums2 == max(sums2))[0][0]\n",
    "        return path2[max_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "request = [30,40,25,50,30,20]\n",
    "index = np.array([*range(1,len(request)+1)])\n",
    "\n",
    "path1 = possiblePath(index)\n",
    "path1 = pathToRequest(path1,request)\n",
    "\n",
    "path2 = possiblePath(index[1:])\n",
    "path2 = pathToRequest(path2,request)\n",
    "\n",
    "request = optimalPath(path1,path2)\n",
    "print(\"The best way to accept the appointments in the given order is: \", request ,\"\\nwith a total duration of :\" , sum(request))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
