{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c681e0cc",
   "metadata": {},
   "source": [
    "# 1. Data collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa82cb9",
   "metadata": {},
   "source": [
    "We have to build a search engine over the \"Top Anime Series\" from the list of MyAnimeList.net. Let's start creating our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cb2b05",
   "metadata": {},
   "source": [
    "## 1.1 Get the list of animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f94936",
   "metadata": {},
   "source": [
    "First of all we need to get the url of the anime that will appear in our list. In order to do that we use BeautifulSoup in combination with the requests library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c939e6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing library needed\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "import ast\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from datetime import datetime as dt\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb131bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we define a function that from the given url of the main page, using BigSoup, gather all the urls \n",
    "#of the animes by filtering the headers of the page. We use start and stop to set the number of pages\n",
    "#we want to have\n",
    "def getUrls(start, stop): \n",
    "\n",
    "    urls = []\n",
    "    for i in range(start, stop):\n",
    "        url = 'https://myanimelist.net/topanime.php?limit='+str(i*50)\n",
    "        r = requests.get(url)\n",
    "        html_content = r.text\n",
    "        soup = bs(html_content, 'lxml')\n",
    "        links = soup.find_all('h3') \n",
    "\n",
    "        for anime in links[:-3]:\n",
    "            if anime.find('a'):\n",
    "                urls.append(anime.find('a')['href'])\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e62780",
   "metadata": {},
   "source": [
    "Once we have a list containing all the URLs, we write them in a text file, so we can avoid this step for the next few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a89231",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving all the urls in a text file\n",
    "f = open(\"topAnime.txt\", 'w', encoding=\"utf8\")\n",
    "f.write('\\n'.join(getUrls(0,400)))\n",
    "f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261dc6e",
   "metadata": {},
   "source": [
    "## 1.2 Crawl animes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782fde8a",
   "metadata": {},
   "source": [
    "Now that we get all the urls, we need to \"convert\" them in html files, so we use the request library to download and save all the anime html pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc4b44c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function take in input the page number, it will then go into the file previously created and, \n",
    "#using the urls of each anime, download the html file corresponding to each of them. \n",
    "# We store the files in subdirectory ordered by the page they appear in, by chunks of 50.\n",
    "def saveHtml(page):\n",
    "#saving the HTML file of page 'page' in the corresponding folder\n",
    "    #creating a subfolder to store the html file of the page 'page'\n",
    "    subfolder = \"downloaded_Html/page_{}\".format(page)\n",
    "    os.makedirs(subfolder)\n",
    "\n",
    "    f = open(\"topAnime.txt\", 'r', encoding=\"utf8\")\n",
    "    lines = f.readlines()[(page-1)*50:(page)*50]\n",
    "    lines = [line.rstrip() for line in lines]\n",
    "    f.close\n",
    "\n",
    "    i = 1+50*(page-1)\n",
    "    #for each link that we read let's create a new .html file\n",
    "    for link in lines:\n",
    "        html = requests.get(link)\n",
    "        \n",
    "        if html.status_code != 200:\n",
    "            while(html.status_code != 200): \n",
    "                check = requests.get(link)\n",
    "        \n",
    "        file_name = '{}/{}.html'.format(subfolder, i)\n",
    "        g = open(file_name, 'w', encoding=\"utf8\")\n",
    "        g.write(html.text)\n",
    "        g.close\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb93cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#as downloading pages take so much time we decided to download 100 pages each\n",
    "for i in range(389,400):\n",
    "    saveHtml(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217799f5",
   "metadata": {},
   "source": [
    "## 1.3 Parse downloaded pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbd7ee5",
   "metadata": {},
   "source": [
    "We are getting closer to our dataset! Now that we've downloaded all the anime pages needed, we can extrapolate the features to create our dataset. In the next cell we define some functions useful to obtain each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e3645e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##title\n",
    "def getTitle(anime):\n",
    "    return anime.strong.contents[0]\n",
    "\n",
    "#tyoe\n",
    "def getType(anime):\n",
    "    return anime.find(text = 'Type:').find_next('a').contents[0]\n",
    "\n",
    "#num episodes\n",
    "def getNumEpis(anime):\n",
    "    if anime.find(text = 'Episodes:').next_element.strip() != \"Unknown\":\n",
    "        return int(anime.find(text = 'Episodes:').next_element.strip())\n",
    "    else :\n",
    "        return []\n",
    "\n",
    "#starting date    \n",
    "def getStart(anime):\n",
    "    date = anime.find(text = 'Aired:').next_element.strip()\n",
    "    \n",
    "    if date != \"Not available\":\n",
    "        if len(date.split(\" to \")[0]) > 8:\n",
    "            return dt.strptime(date.split(\" to \")[0], '%b %d, %Y' )\n",
    "        elif len(date.split(\" to \")[0]) == 4:\n",
    "            return dt.strptime(date.split(\" to \")[0], '%Y' )\n",
    "        elif len(date.split(\" to \")[0]) == 8:\n",
    "            return dt.strptime(date.split(\" to \")[0], '%b %Y' )\n",
    "        else:\n",
    "            return pd.to_datetime(np.NaN, errors='coerce')\n",
    "    else :\n",
    "        return []\n",
    "\n",
    "#ending date   \n",
    "def getEnd(anime):\n",
    "    date = anime.find(text = 'Aired:').next_element.strip()\n",
    "    \n",
    "    if date != \"Not available\":\n",
    "        if len(date)>12 and len(date.split(\" to \")[1]) > 8 and date.split(\" to \")[1] != \"?\":\n",
    "            return dt.strptime(date.split(\" to \")[1], '%b %d, %Y' )\n",
    "        elif len(date)>12 and len(date.split(\" to \")[1]) == 4 and date.split(\" to \")[1] != \"?\":\n",
    "            return dt.strptime(date.split(\" to \")[1], '%Y' )\n",
    "        elif len(date)>12 and len(date.split(\" to \")[1]) == 8 and date.split(\" to \")[1] != \"?\":\n",
    "            return dt.strptime(date.split(\" to \")[1], '%b %Y' )\n",
    "        else:\n",
    "            return pd.to_datetime(np.NaN, errors='coerce')\n",
    "    else :\n",
    "        return []        \n",
    "\n",
    "#members number   \n",
    "def getNumMemb(anime):\n",
    "    animeNumMembers = anime.find(text = 'Members:').next_element\n",
    "    return int(animeNumMembers.replace('n', '').replace(',', '').strip())\n",
    "\n",
    "#score of the anime\n",
    "def getScore(anime):\n",
    "    if anime.find(text = 'Score:').find_next('span').contents[0] != 'N/A':\n",
    "        animeScore = anime.find(text = 'Score:').find_next('span').contents\n",
    "        return float(animeScore[0])\n",
    "    else :\n",
    "        return []         \n",
    "\n",
    "#user votiing    \n",
    "def getUsers(anime):\n",
    "    animeUsers = anime.find(text = 'Score:').find_next('span').find_next('span').contents\n",
    "    if animeUsers[0] != 'Ranked:':\n",
    "        return int(animeUsers[0])\n",
    "    else :\n",
    "        return []      \n",
    "\n",
    "#anime rank\n",
    "def getRank(anime):\n",
    "    animeRank = anime.find(text = 'Ranked:').next_element\n",
    "    if animeRank.replace('\\n', '').strip() != 'N/A':\n",
    "        return int(animeRank.replace('\\n', '').replace('#', '').strip())\n",
    "    else :\n",
    "        return [] \n",
    "\n",
    "#anime Popularity   \n",
    "def getPopularity(anime):\n",
    "    animePopularity = anime.find(text='Popularity:').next_element\n",
    "    return int(animePopularity.replace(\"\\n\",\"\").replace('#', '').strip())\n",
    "\n",
    "#anime description\n",
    "def getDescription(anime):\n",
    "    animeDescription = anime.find(text = 'Synopsis').find_next('p').text\n",
    "    return animeDescription.replace(\"\\n\",\"\")\n",
    "\n",
    "#related anime\n",
    "def getRelated(anime):\n",
    "    table = anime.find(text = 'Related Anime')\n",
    "    animeRelated = []\n",
    "\n",
    "    if(table != None):    \n",
    "        table = table.find_next('table')\n",
    "        table = table.find_all('a')\n",
    "\n",
    "        for el in table:\n",
    "            animeRelated.append(el.text)\n",
    "                \n",
    "    return animeRelated\n",
    "\n",
    "#get the name of the main characters\n",
    "def getCharact(anime):\n",
    "    table = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "    table = table.find_all('table')\n",
    "\n",
    "    animeChar = []\n",
    "\n",
    "    for el in table:\n",
    "        people = el.find_all('h3')\n",
    "        for person in people:\n",
    "            animeChar.append(person.text)\n",
    "        \n",
    "    return animeChar\n",
    "\n",
    "#get the voices of the main characters\n",
    "def getVoices(anime):\n",
    "    table = anime.find(text = 'Characters & Voice Actors').find_next('div')\n",
    "    table = table.find_all('h3')\n",
    "\n",
    "    animeVoices = []\n",
    "\n",
    "    for el in table:\n",
    "        people = el.find_next('table')\n",
    "        for person in people:\n",
    "            animeVoices.append(person.find('a').text)\n",
    "        \n",
    "    return animeVoices\n",
    "\n",
    "#get name and role of the staff\n",
    "def getStaff(anime):\n",
    "    \n",
    "    table = anime.find_all(text = 'Staff')[1].find_next(\"div\", {\"class\": \"detail-characters-list clearfix\"})\n",
    "    animeStaff = []\n",
    "    if(table != None):    \n",
    "        table = table.find_all(\"table\")\n",
    "        for el in table:\n",
    "            x = el.find_all(\"td\")[1]\n",
    "            person = [x.find(\"a\").text, x.find(\"small\").text]\n",
    "            animeStaff.append(person)\n",
    "    \n",
    "    return animeStaff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3081cdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#since the pages are not saved in numerical order in the folder, let’s tidy them up\n",
    "pages = sorted(os.listdir('downloaded_Html')[1:], key = lambda page : int(page.split(\"_\")[1]))\n",
    "\n",
    "title = []\n",
    "typ = []\n",
    "numEpisode = []\n",
    "start = []\n",
    "end = []\n",
    "numMembers = []\n",
    "score = []\n",
    "users = []\n",
    "rank = []\n",
    "popularity = []\n",
    "synopsis = []\n",
    "related = []\n",
    "char = []\n",
    "voices = []\n",
    "staff = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b337c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's extrapolate the features from each anime and store them in lists\n",
    "for page in pages:\n",
    "    htmls = os.listdir('downloaded_Html/{}'.format(page))\n",
    "    for i in range(1,1+len(htmls)):\n",
    "        f = open(\"downloaded_Html/{}/{}.html\".format(page,50*(int(page.split(\"_\")[1])-1)+i), 'r', encoding=\"utf8\")\n",
    "        anime = bs(f, 'lxml')\n",
    "        title.append(getTitle(anime))\n",
    "        typ.append(getType(anime))\n",
    "        numEpisode.append(getNumEpis(anime))\n",
    "        start.append(getStart(anime))\n",
    "        end.append(getEnd(anime))\n",
    "        numMembers.append(getNumMemb(anime))\n",
    "        score.append(getScore(anime))\n",
    "        users.append(getUsers(anime))\n",
    "        rank.append(getRank(anime))\n",
    "        popularity.append(getPopularity(anime))\n",
    "        synopsis.append(getDescription(anime))\n",
    "        related.append(getRelated(anime))\n",
    "        char.append(getCharact(anime))\n",
    "        voices.append(getVoices(anime))\n",
    "        staff.append(getStaff(anime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a283338",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['animeTitle', 'animeType', 'animeNumEpisode','releaseDate', 'endDate', 'animeNumMembers', 'animeScore', 'animeUsers', \n",
    "       'animeRank', 'animePopularity', 'animeDescription', 'animeRelated', 'animeCharacters', 'animeVoices', 'animeStaff']\n",
    "\n",
    "types = {'animeTitle' : 'object', \n",
    "         'animeType' : 'object', \n",
    "         'animeNumEpisode' : 'int64',\n",
    "         'releaseDate' : 'datetime64', \n",
    "         'endDate' : 'datetime64', \n",
    "         'animeNumMembers' : 'int64', \n",
    "         'animeScore' : 'float64',\n",
    "         'animeUsers' : 'int64', \n",
    "         'animeRank' : 'int64',\n",
    "         'animePopularity' : 'int64',\n",
    "         'animeDescription' : 'object',\n",
    "         'animeRelated' : 'object',\n",
    "         'animeCharacters' : 'object',\n",
    "         'animeVoices' : 'object',\n",
    "         'animeStaff' : 'object'}\n",
    "\n",
    "df = pd.DataFrame(columns = col).astype(dtype = types) \n",
    "\n",
    "df.animeTitle = title \n",
    "df.animeType = typ \n",
    "df.animeNumEpisode = numEpisode \n",
    "df.releaseDate = start \n",
    "df.endDate = end \n",
    "df.animeNumMembers = numMembers \n",
    "df.animeScore = score \n",
    "df.animeUsers = users \n",
    "df.animeRank = rank \n",
    "df.animePopularity = popularity \n",
    "df.animeDescription = synopsis \n",
    "df.animeRelated = related \n",
    "df.animeCharacters = char \n",
    "df.animeVoices = voices \n",
    "df.animeStaff = staff "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5268843d",
   "metadata": {},
   "source": [
    "Finally here it is, we got our dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1c3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a .tsv file (tab separated values) where we are going to store all of the informations \n",
    "# for each anime\n",
    "df = df.to_csv(\"data/anime.tsv\", sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d462899a",
   "metadata": {},
   "source": [
    "# 2. Search engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3098563",
   "metadata": {},
   "source": [
    "### Preprocessing Anime Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e03c2ec",
   "metadata": {},
   "source": [
    "In this section we preprocess the Synopsis of the Anime with the following steps:\n",
    "\n",
    "- Stopwords and punctuation removal\n",
    "- Case normalization (we chose to convert everything to lower case)\n",
    "- Lemmatization, i.e., an alternative to Stemming based on *Part-Of-Speech tagging* and a dictionary (english in our case). We preferred it over Stemming because it returns proper words that have a meaning in english rather then cut off portions of those words\n",
    "- Tokenization of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "86576b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\CASA-PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\CASA-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\CASA-\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Import Libraries\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from os.path import expanduser\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a2f6eea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part Of Speech Tagging Step needed for Lemmatization\n",
    "def wordnet_pos_gen(lista): \n",
    "    pos_tags = list(nltk.pos_tag(lista))\n",
    "    tag_dict = {\"J\": wordnet.ADJ, \"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"R\": wordnet.ADV}\n",
    "    t = []\n",
    "    for x in pos_tags:\n",
    "        try:\n",
    "            t.append((x[0], tag_dict[x[1][0]]))\n",
    "        except Exception:\n",
    "            t.append(\n",
    "                (x[0], \"n\"))\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "103d17f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords Removal\n",
    "def stopping(ls, *args):\n",
    "    processed = []\n",
    "    for case in ls:\n",
    "        tok = word_tokenize(case)\n",
    "        stop_words = set(stopwords.words(\"english\"))\n",
    "        stop_words.add(\",\"), stop_words.add(\".\"), stop_words.add(\";\"), stop_words.add(\":\")\n",
    "        res = [x for x in tok if not x in stop_words]\n",
    "        result = \"\"\n",
    "        for x in res:\n",
    "            result = result + \" \" + x\n",
    "        processed.append(result)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "627b2f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatizer\n",
    "def lemma(lista):\n",
    "    out = []\n",
    "    t = wordnet_pos_gen(lista)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    res = [lemmatizer.lemmatize(w[0], w[1]) for w in t]\n",
    "    result = \"\"\n",
    "    for x in res:\n",
    "        result = result + \" \" + x\n",
    "    out.append(result.lower())  #case normalization step here\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d5af6cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-58-ad7c373b87df>:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.animeDescr[y] = [x for x in stopping(lemma(descr_corpus_tokenized[y])) if x != \"\"]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-ad7c373b87df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#Assign the new description to each new row as a list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0my\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manimeDescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manimeDescr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescr_corpus_tokenized\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m#Convert lists into strings for each row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-f971feaeb7c3>\u001b[0m in \u001b[0;36mstopping\u001b[1;34m(ls, *args)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcase\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcase\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mstop_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\";\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\":\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtok\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     21\u001b[0m         return [\n\u001b[0;32m     22\u001b[0m             \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mline_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         ]\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \"\"\"\n\u001b[0;32m    207\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, fileid)\u001b[0m\n\u001b[0;32m    333\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 335\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mFileSystemPathPointer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\compat.py\u001b[0m in \u001b[0;36m_decorator\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     39\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_decorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_py3_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0minit_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwraps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decorator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, _path)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m         \u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No such file or directory: %r\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Tokenization of preprocessed anime descriptions\n",
    "descr_corpus_tokenized = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df = pd.read_csv(\"data/anime.tsv\", sep='\\t')\n",
    "\n",
    "for descr in df.animeDescription:\n",
    "    tokenized_descr = tokenizer.tokenize(descr.lower())\n",
    "    descr_corpus_tokenized.append(tokenized_descr)\n",
    "\n",
    "for s,descr in enumerate(descr_corpus_tokenized):\n",
    "    filtered = []\n",
    "    for token in descr:\n",
    "        if len(token) > 2 and not token.isnumeric():\n",
    "            filtered.append(token)\n",
    "    descr_corpus_tokenized[s] = filtered\n",
    "\n",
    "#Create a new column with the preprocessed text, since we will output the original version    \n",
    "df[\"animeDescr\"]=\"\"\n",
    "\n",
    "#Assign the new description to each new row as a list\n",
    "for y in range(len(df.animeDescription)):\n",
    "    df.animeDescr[y] = [x for x in stopping(lemma(descr_corpus_tokenized[y])) if x != \"\"]\n",
    "\n",
    "#Convert lists into strings for each row     \n",
    "for x in range(len(df.animeDescr)):\n",
    "    s = \"\"\n",
    "    for y in df.animeDescr[x]:\n",
    "        s+=y\n",
    "    df.animeDescr[x] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a547ee1",
   "metadata": {},
   "source": [
    "## 2.1 Conjunctive query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0888bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we have all the information we need, well stored, we need to start the preprocessing part.\n",
    "synopsis = pd.read_csv(\"data/anime.tsv\", sep='\\t', usecols = [\"animeDescription\"])\n",
    "\n",
    "# Using nltk library we gather all the stopword specifying the language we are working with\n",
    "stop_words = set(stopwords.words('english')) \n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53a2e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We then define a function that will make all the preprocessing steps for each row passed to it.\n",
    "# We then store all of this words in a list \n",
    "def row_token(row):\n",
    "    out = list()\n",
    "    tokens = tokenizer.tokenize(row.lower())\n",
    "    for word in tokens:\n",
    "        if word not in stop_words:\n",
    "            out.append(ps.stem(word))\n",
    "    out = list(dict.fromkeys(out))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6194eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "\n",
    "for row in synopsis:\n",
    "    for word in row_token(row):\n",
    "        if word not in stop_words:\n",
    "            total.append(ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab822488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a .csv type file to store the words and use an ID to recognize them\n",
    "voc = {}\n",
    "id = 0\n",
    "for tok in total:\n",
    "    voc[tok] = id\n",
    "    id+=1\n",
    "\n",
    "with open('data/vocabulary.csv', 'w', newline = '',encoding = 'utf8') as f:\n",
    "    fieldnames = ['Word', 'term_id']\n",
    "    w = csv.DictWriter(f, fieldnames = fieldnames,)\n",
    "    w.writeheader()\n",
    "    for key in voc:\n",
    "        w.writerow({\"Word\": key, 'term_id': voc[key]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "225708e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>term_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>horrif</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>alchemi</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>experi</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>goe</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wrong</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Word  term_id\n",
       "0   horrif        0\n",
       "1  alchemi        1\n",
       "2   experi        2\n",
       "3      goe        3\n",
       "4    wrong        4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = pd.read_csv(\"data/vocabulary.csv\")\n",
    "vocabulary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c2278254",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setDict(d,l):\n",
    "    for i in l:\n",
    "        d[i] = []\n",
    "    return d\n",
    "\n",
    "def listToString(l):\n",
    "    return ' '.join([str(elem) for elem in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da729ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is to create the structure for the Inverted Index file\n",
    "inverted_index = {}\n",
    "inverted_index = setDict(inverted_index, vocabulary.term_id)\n",
    "\n",
    "for i in range(len(synopsis)):\n",
    "    s = listToString(row_token(synopsis[i]))\n",
    "    for j in range(len(vocabulary.Word)):\n",
    "        if s.find(str(vocabulary.Word[j])) != -1:\n",
    "            inverted_index[vocabulary.term_id[j]].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1528f373",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we just create the \"inverted_index.csv\" that we will be using in our search engine\n",
    "with open('data/inverted_index.csv', 'w', newline = '',encoding = 'utf8') as f:\n",
    "    fieldnames = ['term_id', 'document']\n",
    "    w = csv.DictWriter(f, fieldnames = fieldnames,)\n",
    "    w.writeheader()\n",
    "    for key in inverted_index:\n",
    "        w.writerow({\"term_id\": key, 'document': inverted_index[key]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e56dd49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term_id</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[0, 82, 99, 151, 318, 504, 506, 529, 547, 663,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 166, 394, 455, 526, 1318, 1488, 1579, 1856...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 13, 23, 39, 61, 65, 70, 72, 74, 91, 93, 96...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[0, 27, 55, 91, 167, 224, 299, 301, 339, 355, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[0, 12, 22, 37, 127, 204, 223, 331, 352, 409, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   term_id                                           document\n",
       "0        0  [0, 82, 99, 151, 318, 504, 506, 529, 547, 663,...\n",
       "1        1  [0, 166, 394, 455, 526, 1318, 1488, 1579, 1856...\n",
       "2        2  [0, 13, 23, 39, 61, 65, 70, 72, 74, 91, 93, 96...\n",
       "3        3  [0, 27, 55, 91, 167, 224, 299, 301, 339, 355, ...\n",
       "4        4  [0, 12, 22, 37, 127, 204, 223, 331, 352, 409, ..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = pd.read_csv(\"data/inverted_index.csv\")\n",
    "index.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01925fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchWord(i):\n",
    "    vocabulary = pd.read_csv(\"data/vocabulary.csv\")\n",
    "    return i in list(vocabulary.Word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b9a84a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define the function whom, taking in input a query and looking in the files needed, \n",
    "# look up for the similarities.\n",
    "def searchEngine(query):\n",
    "    vocabulary = pd.read_csv(\"data/vocabulary.csv\")\n",
    "    index = pd.read_csv(\"data/inverted_index.csv\")\n",
    "    stemQuery = row_token(query)\n",
    "    \n",
    "    if all([searchWord(i) for i in stemQuery]):\n",
    "        ind = [np.where(vocabulary.Word == x)[0][0] for x in stemQuery]\n",
    "        aux = [set(ast.literal_eval(index.document[vocabulary.term_id[i]])) for i in ind]\n",
    "        return set.intersection(*aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be49b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, this function will use the precedent ones to return the tables of the results of the \n",
    "# search engine, composed by: the title of the Anime, its description and its URL\n",
    "def documentFinder(query):\n",
    "    anime = pd.read_csv(\"data/anime.tsv\", sep='\\t', usecols = [\"animeTitle\",\"animeDescription\"])\n",
    "    f = open(\"data/topAnime.txt\", 'r', encoding=\"utf8\")\n",
    "    topAnimeUrls = f.readlines()\n",
    "    col = ['animeTitle', \"animeDescription\", \"Url\"]\n",
    "    df = pd.DataFrame(columns = col)\n",
    "    ind = searchEngine(query)\n",
    "        \n",
    "    df.animeTitle = [anime.animeTitle[i] for i in ind]\n",
    "    df.animeDescription = [anime.animeDescription[i] for i in ind]\n",
    "    df.Url = [topAnimeUrls[i] for i in ind]\n",
    "    \n",
    "    f.close\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39c1a117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>animeTitle</th>\n",
       "      <th>animeDescription</th>\n",
       "      <th>Url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Duel Masters Victory</td>\n",
       "      <td>Duel Masters Victory is the 7th season of the ...</td>\n",
       "      <td>https://myanimelist.net/anime/10524/Duel_Maste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tetsujin 28-gou</td>\n",
       "      <td>Dr.Haneda was developing experimental giant ro...</td>\n",
       "      <td>https://myanimelist.net/anime/2686/Tetsujin_28...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nano Invaders</td>\n",
       "      <td>Nano Invaders follows the eye-popping adventur...</td>\n",
       "      <td>https://myanimelist.net/anime/27943/Nano_Invad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Evangelion: 1.0 You Are (Not) Alone</td>\n",
       "      <td>In a post-apocalyptic world, the last remainin...</td>\n",
       "      <td>https://myanimelist.net/anime/2759/Evangelion_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chang An Huan Jie</td>\n",
       "      <td>In Zhenguan’s first year, the sects led by Tia...</td>\n",
       "      <td>https://myanimelist.net/anime/45600/Chang_An_H...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>Hyakujuu-Ou GoLion</td>\n",
       "      <td>Golion, a powerful sentient robot, abuses his ...</td>\n",
       "      <td>https://myanimelist.net/anime/1448/Hyakujuu-Ou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>Detonator Orgun</td>\n",
       "      <td>Fleeing from his own race, Orgun—an alien bein...</td>\n",
       "      <td>https://myanimelist.net/anime/1178/Detonator_O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Elysium</td>\n",
       "      <td>In In the year 2113 A.D., a team of scientists...</td>\n",
       "      <td>https://myanimelist.net/anime/2941/Elysium\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>Densetsu Kyojin Ideon: Sesshoku-hen</td>\n",
       "      <td>Scouring the universe in pursuit of knowledge,...</td>\n",
       "      <td>https://myanimelist.net/anime/2760/Densetsu_Ky...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Dennou Sentai Voogie's★Angel</td>\n",
       "      <td>A hundred years after Earth is invaded by alie...</td>\n",
       "      <td>https://myanimelist.net/anime/2453/Dennou_Sent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             animeTitle  \\\n",
       "0                  Duel Masters Victory   \n",
       "1                       Tetsujin 28-gou   \n",
       "2                         Nano Invaders   \n",
       "3   Evangelion: 1.0 You Are (Not) Alone   \n",
       "4                     Chang An Huan Jie   \n",
       "..                                  ...   \n",
       "64                   Hyakujuu-Ou GoLion   \n",
       "65                      Detonator Orgun   \n",
       "66                              Elysium   \n",
       "67  Densetsu Kyojin Ideon: Sesshoku-hen   \n",
       "68         Dennou Sentai Voogie's★Angel   \n",
       "\n",
       "                                     animeDescription  \\\n",
       "0   Duel Masters Victory is the 7th season of the ...   \n",
       "1   Dr.Haneda was developing experimental giant ro...   \n",
       "2   Nano Invaders follows the eye-popping adventur...   \n",
       "3   In a post-apocalyptic world, the last remainin...   \n",
       "4   In Zhenguan’s first year, the sects led by Tia...   \n",
       "..                                                ...   \n",
       "64  Golion, a powerful sentient robot, abuses his ...   \n",
       "65  Fleeing from his own race, Orgun—an alien bein...   \n",
       "66  In In the year 2113 A.D., a team of scientists...   \n",
       "67  Scouring the universe in pursuit of knowledge,...   \n",
       "68  A hundred years after Earth is invaded by alie...   \n",
       "\n",
       "                                                  Url  \n",
       "0   https://myanimelist.net/anime/10524/Duel_Maste...  \n",
       "1   https://myanimelist.net/anime/2686/Tetsujin_28...  \n",
       "2   https://myanimelist.net/anime/27943/Nano_Invad...  \n",
       "3   https://myanimelist.net/anime/2759/Evangelion_...  \n",
       "4   https://myanimelist.net/anime/45600/Chang_An_H...  \n",
       "..                                                ...  \n",
       "64  https://myanimelist.net/anime/1448/Hyakujuu-Ou...  \n",
       "65  https://myanimelist.net/anime/1178/Detonator_O...  \n",
       "66       https://myanimelist.net/anime/2941/Elysium\\n  \n",
       "67  https://myanimelist.net/anime/2760/Densetsu_Ky...  \n",
       "68  https://myanimelist.net/anime/2453/Dennou_Sent...  \n",
       "\n",
       "[69 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"race alien\"\n",
    "documentFinder(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2376770",
   "metadata": {},
   "source": [
    "## 2.2 Conjunctive query & Ranking score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1404b3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pickle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from numpy import inner\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e4f89b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'animeDescr'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-37-658de2a54114>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprocessed_descr_corpus_tokenized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdescr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manimeDescr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mtokenized_descr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdescr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mprocessed_descr_corpus_tokenized\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_descr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5463\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5464\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5465\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5467\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'animeDescr'"
     ]
    }
   ],
   "source": [
    "#Tokenization of Preprocessed Text\n",
    "processed_descr_corpus_tokenized = []\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for descr in df.animeDescr:\n",
    "    tokenized_descr = tokenizer.tokenize(descr.lower())\n",
    "    processed_descr_corpus_tokenized.append(tokenized_descr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168643eb",
   "metadata": {},
   "source": [
    "### 2.2.1 Built an Inverted Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11178727",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency of words for each Document\n",
    "tf={}\n",
    "for i in range(len(processed_descr_corpus_tokenized)):\n",
    "    tf[i] = {}\n",
    "    for w in processed_descr_corpus_tokenized[i]:\n",
    "        if w not in tf[i]:\n",
    "            tf[i][w]=0\n",
    "        tf[i][w]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b56e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Frequency of all words present in the corpus of documents\n",
    "idf={}\n",
    "for l in processed_descr_corpus_tokenized:\n",
    "    for w in l:\n",
    "        if w not in idf:\n",
    "            idf[w]=0\n",
    "        idf[w]+=1\n",
    "\n",
    "for key in idf:\n",
    "    idf[key] =  np.log10(len(df.animeTitle)/idf[key])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d56043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inverted Index containing TF-IDF score\n",
    "tfidf_invIndex = {}\n",
    "for j in range(len(df.animeTitle)):\n",
    "    for w in tf[j]:\n",
    "        if w not in list(tfidf_invIndex.keys()):\n",
    "            tfidf_invIndex[w] = []\n",
    "        tfidf_invIndex[w].append((j, round(tf[j][w] / idf[w], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda80446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saved Inverted Index as a pickle file   \n",
    "with open('vocabularyFinal.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf_invIndex, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7fbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load previously saved Inverted Index\n",
    "with open('vocabularyFinal.pkl', 'rb') as f:\n",
    "    tfidf_invIndex = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f9a9de",
   "metadata": {},
   "source": [
    "### 2.2.2 Retrieve the most similar documents to the Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d56ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sample Query\n",
    "query = \"sword\".split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17825cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency of words in the Query\n",
    "tfQ= {}\n",
    "for w in query:\n",
    "    if w not in list(tfQ.keys()):\n",
    "        tfQ[w]=0\n",
    "    tfQ[w]+=1\n",
    "    \n",
    "#Compute TFIDF values of each word in the Query    \n",
    "tfIdfQ = {}\n",
    "for w in list(tfQ.keys()):\n",
    "    tfIdfQ[w] =   tfQ[w]/np.log10(len(df.animeTitle)/idf[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bb3b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select most similar documents to the query (i.e,. those containing ALL the words of the query)\n",
    "rightDocs = []\n",
    "for i in range(len(df.animeTitle)):\n",
    "    for w in query:\n",
    "        if w in list(tf[i].keys()):\n",
    "            if w==query[-1]:\n",
    "                rightDocs.append(i)\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1549260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Retrieve TFIFD values of similar documents' words from the the InvertedIndex\n",
    "for i in rightDocs:\n",
    "    for word in tf[i]:\n",
    "        tfidfD = [x[1] for x in tfidf_invIndex[word] if x[0]==i][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39d1bb",
   "metadata": {},
   "source": [
    "Once we have TFIDF values for every word in both the *similar* documents and the query, we need to return the top-k ones based on the cosine similarity. For this, we create a vector representation of words, using the words as variables in the same exact order. Then, we can compute the cosine between the vectors as an approximation for similarity.\n",
    "\n",
    "We push newly computed cosine scores into a heap with k nodes, popping the minima when exceeding k nodes.\n",
    "\n",
    "The choice of k is arbitrary, we may avoid to pick a number that is either too small (e.g., 1, because that result is not the one that the user was searching we have failed) or too big (returning too many results may be cumbersome in terms of speed, and a user most likely might need just a few). Anything in between is ok. Furthermore, can set it to the exact number of results that would fit in a single page in our browser (with the assumption that the average user will rarely go the second page, let alone the n-th). In the case of google search engine for example that number is set to 10, so we made the same choice in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa285b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the most similar k documents, with k chosen as 10 to match the number of results that fit in a google page, \n",
    "#in terms of cosine similarity, handled with a heap data structure\n",
    "scoreHeap, k = [], 10\n",
    "heapq.heapify(scoreHeap)\n",
    "for i in rightDocs:\n",
    "    vector = list(set([x for x in list(tfIdfQ.keys())] + [x for x in list(tf[i].keys())]))\n",
    "    vectorQ = dict.fromkeys(vector, 0)\n",
    "    for key in tfIdfQ:\n",
    "        vectorQ[key] = tfIdfQ[key]\n",
    "    vectorD = dict.fromkeys(vector, 0)\n",
    "    for word in tf[i]:\n",
    "        vectorD[word] = [x[1] for x in tfidf_invIndex[word] if x[0] == i][0]\n",
    "    cosine = np.inner(list(vectorQ.values()), list(vectorD.values())) / (norm(list(vectorQ.values())) * norm(list(vectorD.values())))  # Best\n",
    "    heapq.heappush(scoreHeap,(i,cosine))\n",
    "    if len(scoreHeap)==k+1:\n",
    "        heapq.heappop(scoreHeap)\n",
    "    scoreHeap  = [x for x in scoreHeap if float(x[1])!=0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31ffa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scoreHeap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a2af07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Output formatting\n",
    "out = []\n",
    "for x in range(10):\n",
    "    id = scoreHeap[x][0]\n",
    "    title = df.animeTitle.iloc[id]\n",
    "    des = df.animeDescription[id]\n",
    "    url = \"\"\n",
    "    sim = scoreHeap[x][1]\n",
    "    out.append((id,title,des,url,sim))\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906ab63",
   "metadata": {},
   "source": [
    "# 3. Define a new score!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434408d0",
   "metadata": {},
   "source": [
    "Our new metric to rank anime, since it must be created using existing variables, is made of 4 components: \n",
    "- Cosine Similarity (previously computed)\n",
    "- Number of members for each anime (corresponding to the number of users that have added the anime to the list) \n",
    "- Number of Users for each anime (corresponding to the number of users that gave a vote to the anime)\n",
    "- Anime score (a vote ranging from 1 to 10 that users of the website can assign to each anime)\n",
    "\n",
    "The cosine similarity was a score ranging already from 0 to 1 (theoretically the cosine would range from -1 to 1, but in this setting i.e. word frequencies, the support is only made of positive numbers), while the others weren't and thus were normalized dividing by their respective maximum value of the dataset.\n",
    "\n",
    "These four quantities were aggregated in such a way to obtain once again a similarity score (hence ranging from 0 to 1), trying to penalize the cosine the least with respect to the other three variables. From our data, we have noticed there are just few limit cases going a bit above one (e.g., top-3 anime for popularity or score, assuming a oddly high cosine value), we tackled the problem setting the upper bound to 1 (through a lambda function).    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ebab99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find highest value for variables that need to be normalized\n",
    "largestMemb = max(df.animeNumMembers) \n",
    "largestUsers =  max(df.animeUsers.apply(lambda x: pd.to_numeric(x, errors='coerce')))\n",
    "highestVote =  max(df.animeScore.apply(lambda x: pd.to_numeric(x, errors='coerce')))\n",
    "\n",
    "#Normalize variable in range [0,1]\n",
    "df[\"normMemb\"] = np.array(df.animeNumMembers) / largestMemb\n",
    "df[\"normUsers\"] = np.array(df.animeUsers.apply(lambda x: pd.to_numeric(x, errors='coerce'))) / largestUsers\n",
    "df[\"normVote\"] = np.array(df.animeScore.apply(lambda x: pd.to_numeric(x, errors='coerce'))) / highestVote\n",
    "\n",
    "#Upper Bound to 1 for cases exceeding 1\n",
    "cap = lambda x: 1.0 if x>1 else x\n",
    "cosine = 1231231 # 0.80\n",
    "score = cap(4/5 * (cosine) + 1/10 * (df.normMemb[0] + df.normUsers[0] + df.normVote[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c985845",
   "metadata": {},
   "source": [
    "# 4. Understanding the anime's reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c787c8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Codice Vecchio da cambiare\n",
    "\n",
    "#Sample Query\n",
    "query = \"death note\"\n",
    "\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import operator\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df[\"sentiment_score\"] = df.animeDescription.apply(lambda x: sia.polarity_scores(x)[\"compound\"])\n",
    "\n",
    "df[\"sentiment\"] = np.select([df[\"sentiment_score\"] < 0, df[\"sentiment_score\"] == 0, df[\"sentiment_score\"] > 0],\n",
    "                           ['neg', 'neu', 'pos'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005960bb",
   "metadata": {},
   "source": [
    "# 5. Algorithmic question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a069b10",
   "metadata": {},
   "source": [
    "We are asked by a personal trainer who has a back-to-back sequence of requests for appointments, to provide a schedule that maximizes the total length of the accepted appointments avoiding consecutive ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8ce40f",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src = \"images/images7.png\" width = \"500px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80c68bb",
   "metadata": {},
   "source": [
    "Our first idea is to analyze with a tree all the possible combination of the possible choices. As we can see in the photo below, until we have an array of length 5, the possible choices are forced:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182076f0",
   "metadata": {},
   "source": [
    "![image4](images/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916d113",
   "metadata": {},
   "source": [
    "Now that we want to put the fifth element of the array, we can take into account the possibility of appending that directly to the root, that is the first element of the array, but we realize that it makes no sense because:\n",
    "- time is always positive;\n",
    "- the fifth element can be appended to the third element since they are not consecutive\n",
    "\n",
    "so when we want to append the fifth element, since we want to get maximizes the amount of time of the appointment, we put it under the third one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00490891",
   "metadata": {},
   "source": [
    "![image2](images/image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77d437f",
   "metadata": {},
   "source": [
    "In conclusion we get a binary tree of the shape:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca424dfe",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src = \"images/image6.png\" width = \"500px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95029d9c",
   "metadata": {},
   "source": [
    "So in order to solve the problem assigned by the personal trainer we will have to build this type of trees. In particular we note that there are only two possible distinct cases the tree rooted in the first element and the one rooted in the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d003478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create all the possibile path from 1 to n and we save it in lists\n",
    "def possiblePath(index):\n",
    "    lists = [[index[0]]]\n",
    "    n = 2\n",
    "    \n",
    "    while n < len(index):\n",
    "        for l in lists:\n",
    "            # we add the element n+1\n",
    "            if(n - np.where(index == l[-1])[0][0] == 2):\n",
    "                l.append(index[n])\n",
    "            # we add the element n+2\n",
    "            if len(l)>1 and (n - np.where(index == l[-2])[0][0] == 3):\n",
    "                aux = l[:-1]\n",
    "                aux.append(index[n]) \n",
    "                if aux not in lists:\n",
    "                    lists.append(aux)\n",
    "        n +=1\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9b50fda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the path of index obtained in the previous function \n",
    "# we  trasform each index in the corrispondent request value\n",
    "def pathToRequest(path, request):\n",
    "    \n",
    "    for l in path:\n",
    "        for i in range(len(l)):\n",
    "            ind = l[i]-1\n",
    "            l[i] = request[ind]\n",
    "            \n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "152f3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seraching for the best path between the 1-rooted tree vs the 2-rooted tree\n",
    "def optimalPath(path1,path2):\n",
    "    \n",
    "    sums1 = np.array([sum(l) for l in path1])\n",
    "    sums2 = np.array([sum(l) for l in path2])\n",
    "    \n",
    "    if(max(sums1)>=max(sums2)):\n",
    "        max_value = np.where(sums1 == max(sums1))[0][0]\n",
    "        return path1[max_value]\n",
    "    else:\n",
    "        max_value = np.where(sums2 == max(sums2))[0][0]\n",
    "        return path2[max_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "699e8b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best way to accept the appointments in the given order is:  [40, 50, 20] \n",
      "with a total duration of : 110\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "request = [30,40,25,50,30,20]\n",
    "index = np.array([*range(1,len(request)+1)])\n",
    "\n",
    "path1 = possiblePath(index)\n",
    "path1 = pathToRequest(path1,request)\n",
    "\n",
    "path2 = possiblePath(index[1:])\n",
    "path2 = pathToRequest(path2,request)\n",
    "\n",
    "request = optimalPath(path1,path2)\n",
    "print(\"The best way to accept the appointments in the given order is: \", request ,\"\\nwith a total duration of :\" , sum(request))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
